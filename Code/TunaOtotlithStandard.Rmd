---
title: "Big Eye Tuna Otolith Micro-Chemistry"
author: "Jerelle Jesse"
output:
    html_document: 
    toc: true
    toc_float: true
    code_folding: hide
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
# Access GMRI CSS Style
library(gmRi)
gmRi::use_gmri_style_rmd(css_file = "gmri_rmarkdown.css")

library(here)
library(dplyr)
library(tidyr)
library(stringr)
library(gtools)
library(purrr)
library(tm)
library(ggplot2)
library(ggrepel)
```

```{r}
# load in data
otolith <- read.csv(here("Data/ICPMS master_071922.csv"))
head(otolith)
str(otolith)

#set up plotting theme
themeo <-theme_classic()+
  theme(strip.background = element_blank(),
        panel.grid.major = element_line(colour = "transparent"), 
        panel.grid.minor = element_line(colour = "transparent"),
        axis.line = element_blank(),
        axis.title = element_text(color = "black", size = 8),
        axis.ticks.length=unit(-0.1, "cm"),
        axis.ticks = element_line(color = "black", size = .25),
        axis.ticks.x = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.y = element_line(color = "black"),
        panel.border = element_rect(colour = "black", fill=NA, size=.5),
        panel.spacing = unit(.5, "lines") )
```

### Step 1: Filter out "bad" data

```{r}
#remove flagged data
otolith_filter <- filter(otolith, Cut.Out ==0)

#remove columns we don't need and convert to long format
otolith_select <- dplyr::select(otolith_filter, Plate, ID, ID2,
                         X7Li,
                         X25Mg,
                         X43Ca,
                         X55Mn,
                         X63Cu,
                         X68Zn,
                         X88Sr,
                         X138Ba)%>%
                tidyr::gather(element, measurement, X7Li:X138Ba)


```

## Step 2: Calculate Standardization

-   average for blanks and standards

-   each sample separate (laser hits)

```{r}
#read in data with bad data filtered out
data <- read.csv(here("Data/ICPMS master_071922.csv"))
#changing plate numbers so the data can be sorted into the correct order without the letters
data$Plate[data$Plate=="1b"] <- 1
data$Plate[data$Plate=="3d"] <- 3
data$Plate[data$Plate=="5b"] <- 5
data$Plate[data$Plate=="6i"] <- 6
data$Plate[data$Plate=="7b"] <- 7
data$Plate[data$Plate=="8b"] <- 8

data$Plate <- as.numeric(data$Plate)

#filtering out bad data, renaming columns, and making tidy
data_filter <- filter(data, Cut.Out ==0)%>%
  rename(Li7= X7Li, Mg25=X25Mg, Ca43=X43Ca, Mn55=X55Mn, Cu63=X63Cu, Zn68=X68Zn, Sr88=X88Sr, Ba138=X138Ba)%>%
  gather(element, measurement, Li7:Ba138)

data_select <- select(data_filter, Plate, Type, ID, Time,element,measurement)


#average only the blanks
avg_data <- filter(data_select, Type=="Blank")%>%
  group_by(Plate, element, Type, ID)%>%
  summarise_all(mean)

samples <- filter(data_select, Type!="Blank")%>%
  full_join(avg_data)



#order each plate x sample
order_data <- group_by(samples, Plate, element)%>%
  arrange(Time, .by_group=T)%>%
  mutate(order= match(Time, unique(Time)))%>%
  ungroup()


#blank corrected
#grab two surrounding blanks from sample/ standard and average them subtract that number from the sample/ standard

# this is not working correctly right now- sometimes two samples with no blank inbetween- need to edit code like below to search for closest blank and use that value #

plate <- sort(unique(order_data$Plate))
blank_data <- NULL

for (i in 1:length(plate)) {
  each_plate <- filter(order_data, Plate == plate[i])
    #each_sample <- dplyr::select(each_plate,Li7:Ba138)
  
    diff <- group_by(each_plate,element)%>%
      mutate(blank= na_if(lag(measurement * (Type== "Blank")),0))%>%
      fill(blank)%>%
      mutate(diff=measurement-blank)
    blank_data <- bind_rows(blank_data, diff)
}
#((lag(each_sample)+lead(each_sample))/2)

## add the descriptive info back in
# merge_info <- select(order_data, 1:4,13)%>%
#   arrange(Plate)%>%
#   bind_cols(blank_data)

#only want the blank corrected samples and standards
# no longer need the blanks- also can take out preablation 
blank_samples <- filter(blank_data, Type!="Blank")%>%
           filter(Type !="Preablation")%>%
            select(! c(measurement, blank))#%>%
          spread(element, diff)


#calculate standard intensity (avg Ca/ avg element)
std_int <- filter(blank_samples, Type=="Standard")%>%
  group_by(Plate, element)%>%
  summarise(avg_std= mean(diff, na.rm=T))%>%
          spread(element, avg_std)

std_ratio <- mutate(std_int,Li= Ca43/Li7,
                Mg = Ca43/Mg25,
                Ca = Ca43/Ca43,
                Mn = Ca43/Mn55,
                Cu = Ca43/Cu63,
                Zn = Ca43/Zn68,
                Sr = Ca43/Sr88,
                Ba = Ca43/Ba138)%>%
         select(!Ba138:Zn68)%>%
  gather(element, std, -Plate)

element <- c("Li", "Mg", "Ca", "Mn", "Cu", "Zn", "Sr", "Ba")
conc <- c(0.953025585, 7.682610043, 1, 1.037460238, 0.200804264, 0.18053357, 8.20395653, 0.045453)
concentration <- data.frame(element, conc)

std_conc <- full_join(std_ratio, concentration)

#calculate chem ratio
ratio <- spread(blank_samples, element, diff)%>%
          filter(Type=="Sample")%>%
         mutate(Li= Li7/Ca43,
                Mg = Mg25/Ca43,
                Ca = Ca43/Ca43,
                Mn = Mn55/Ca43,
                Cu = Cu63/Ca43,
                Zn = Zn68/Ca43,
                Sr = Sr88/Ca43,
                Ba = Ba138/Ca43)%>%
  select(!Ba138:Zn68)%>%
  arrange(Plate, order)%>%
  gather(element, measurement, -c(1:5))

#finish standardization
# multiply and chemical ratio times the standard intensity and the standard concentration (this is a standard value for each element)

results <-  full_join(ratio, std_conc)%>%
  mutate(results= measurement*std*conc)%>%
  select(!order)


#add desciptive info back in to samples
info <- select(data_filter,Plate, Type, ID,Time,element, Age.of.Sample, Age, Year.Class, Region, Weight, Sex )%>%
  filter(Type=="Sample")%>%
  mutate(element= removeNumbers(element))

merge <- right_join(info,results)%>%
  group_by(Plate, element)%>%
  arrange(Time, .by_group=T)%>%
  ungroup()

#save data
#write.csv(merge, file=here("Data/Clean/BET_standard_071922.csv"))
```

## Step 3: Anomaly detection

```{r}
# anomaly detection is not working yet (modified from Sarah Becker)#
data <- read.csv(here("Data/Clean/BET_standard_071922.csv"))
oto_samples <- unique(data$ID)
oto_elements<- unique(data$element)
oto_anoms <- NULL


for( i in 1:length(oto_samples)){
  #   subset samples
  sub_samples <- subset(data, ID == oto_samples[i] )
  
  for (j in 1:length(oto_elements)){
    # subset elements
    sub_elements <- subset(sub_samples, element == oto_elements[j] )
    elements <- sub_elements$results
    
     trend = runmed(elements, 5)
    detrend = elements - as.vector(trend)
    
    min = mean(detrend, na.rm = T) - 5*sd(detrend, na.rm = T)
    max = mean(detrend, na.rm = T) + 5*sd(detrend, na.rm = T)
    
    #find anomaly
    position = data.frame(id=seq(1, length(detrend)), value=detrend)
    anomalyH = position[position$value > max, ]
    anomalyH = anomalyH[!is.na(anomalyH$value), ]
    anomalyL = position[position$value < min, ]
    anomalyL = anomalyL[!is.na(anomalyL$value),]
    anomaly = data.frame(id=c(anomalyH$id, anomalyL$id),
                         value=c(anomalyH$value, anomalyL$value))
    real = data.frame(id=seq(1, length(elements)), value=elements)
    
    #extract anomalies
    realAnomaly = real[anomaly$id, ]
    
    #add indexing columns back in
        if(nrow(realAnomaly) == 0)next
    realAnomaly$ID <- sub_elements$ID[i] %>% as.character()
    realAnomaly$element <- sub_elements$element[j] %>% as.character()
    
    #bind to empty dataframe
    oto_anoms <- rbind(oto_anoms, realAnomaly)
  }
}

# merge 
# first need to go back and add an index value  
data_id <- group_by(data,ID, element) %>% 
  dplyr::mutate(id = seq(1:n())) 

data_anom <- left_join(data_id, oto_anoms, by = c("ID", "element", "id")) 

#save data
#write.csv(data_anom, here("Data/Clean/BET_anom_071922.csv"))


```

Plot the anomalies

```{r}
data_anom <- read.csv(here("Data/Clean/BET_anom_071922.csv"))[-1]%>%
  filter(element !="CaCa")

uniq_samples <- unique(data_anom$ID)
#### RUN LOOP
for (i in uniq_samples) {
  
  plot <- ggplot(data= filter(data_anom, ID == i)) + 
    geom_line(aes(x=Time, y=results))+
    geom_point(aes(x=Time, y=value), color = "red")+
    facet_wrap(~ element,ncol=1, scales = "free_y")+
    ggtitle(i)+
    themeo
  
    ggsave(plot, path=here("Results/Figures/Anomalies"), file=paste0("plot_", i,".png"), width=10, height=25, units="cm")
}

```

remove anomalies

```{r}
#remove anomalies and save

data_anom_removal <- filter(data_anom, is.na(value))%>%
  select(!c("id", "value"))

#write.csv(data_anom_removal, here("Data/Clean/BET_anom_removal_071922.csv"))

```

## Step 4: Plot the results

```{r}
#plots beore anomaly detection- definitnely see some anomalies
data <- read.csv(here("Data/Clean/BET_standard_071922.csv"))[-1]

# make age break column for plots
oto_samples <- unique(data$ID)
transects_age_break <- NULL

##### RUN LOOP (modified from Sarah Becker)
for( i in 1:length(oto_samples)){
  #   subset samples
  sub_samples <- subset(data, ID == oto_samples[i] )%>%
    mutate(id=seq(1:n()))

  #ID where age breaks occur
  age_break <-  as.data.frame(which(sub_samples$Age.of.Sample!= dplyr::lag(sub_samples$Age.of.Sample)))
  #rename column to not be crazy
  age_break<- age_break %>% dplyr::rename("id" = "which(sub_samples$Age.of.Sample != dplyr::lag(sub_samples$Age.of.Sample))") 
  #need to merge by id, then have a separate column to store the values
  age_break <- age_break %>%  mutate(age_diff = id)
  #do the merge
  sub_samples <- left_join(sub_samples, age_break, by = "id")
  # create new column that shows where breaks are by dist_from_core
  sub_samples$age_break_dist <- ifelse(!is.na(sub_samples$age_diff), sub_samples$Time, NA)
  #bind to empty dataframe
  transects_age_break <- rbind(transects_age_break, sub_samples)
}

#write.csv(transects_age_break, here("Data/Clean/BET_age_breaks_071922.csv"))
```

```{r}

#data with age breaks
data <- read.csv(here("Data/Clean/BET_age_breaks_071922.csv"))[-1]%>%
  select(!c("age_diff", "id"))%>%
  filter(element!="Ca")

SampleID <- unique(data$ID)

for (i in SampleID) {
  temp_data <- filter(data, ID==i)
  
  plot <- ggplot(data=temp_data, aes(x=Time, y=results))+
    geom_line()+
    geom_point(size=1)+
    geom_vline(aes(xintercept=age_break_dist), linetype="dashed")+
    geom_text(aes(x = age_break_dist, y = +Inf,  label = Age.of.Sample),vjust=1, hjust=1)+
    facet_wrap(~element,ncol=1, scales = "free")+
    ggtitle(i)+ 
    labs(y="Element ratio", x="")+
    themeo
  plot
  ggsave(plot, path=here("Results/Figures/transects"), file=paste0("plot_", i,".png"), width=10, height=25, units="cm")
}
```

After anomaly detection

```{r}
# age breaks
data <- read.csv(here("Data/Clean/BET_anom_removal_071922.csv"))[-1]

# make age break column for plots
oto_samples <- unique(data$ID)
transects_age_break <- NULL

##### RUN LOOP (modified from Sarah Becker)
for( i in 1:length(oto_samples)){
  #   subset samples
  sub_samples <- subset(data, ID == oto_samples[i] )%>%
    mutate(id=seq(1:n()))

  #ID where age breaks occur
  age_break <-  as.data.frame(which(sub_samples$Age.of.Sample!= dplyr::lag(sub_samples$Age.of.Sample)))
  #rename column to not be crazy
  age_break<- age_break %>% dplyr::rename("id" = "which(sub_samples$Age.of.Sample != dplyr::lag(sub_samples$Age.of.Sample))") 
  #need to merge by id, then have a separate column to store the values
  age_break <- age_break %>%  mutate(age_diff = id)
  #do the merge
  sub_samples <- left_join(sub_samples, age_break, by = "id")
  # create new column that shows where breaks are by dist_from_core
  sub_samples$age_break_dist <- ifelse(!is.na(sub_samples$age_diff), sub_samples$Time, NA)
  #bind to empty dataframe
  transects_age_break <- rbind(transects_age_break, sub_samples)
}

#write.csv(transects_age_break, here("Data/Clean/BET_age_breaks_anom_071922.csv"))
```

```{r}
#plot
#data with age breaks
data <- read.csv(here("Data/Clean/BET_age_breaks_anom_071922.csv"))[-1]%>%
  select(!c("age_diff", "id"))%>%
  filter(element!="Ca")

SampleID <- unique(data$ID)

for (i in SampleID) {
  temp_data <- filter(data, ID==i)
  
  plot <- ggplot(data=temp_data, aes(x=Time, y=results))+
    geom_line()+
    geom_point(size=1)+
    geom_vline(aes(xintercept=age_break_dist), linetype="dashed")+
    geom_text(aes(x = age_break_dist, y = +Inf,  label = Age.of.Sample),vjust=1, hjust=1)+
    facet_wrap(~element,ncol=1, scales = "free")+
    ggtitle(i)+ 
    labs(y="Element ratio", x="")+
    themeo
  plot
  
  ggsave(plot, path=here("Results/Figures/Anom removed transects"), file=paste0("plot_", i,".png"), width=10, height=25, units="cm")
}
```

## Step 5: Analyze

-   Differences in chemistry by year class, area of capture/ year of capture (and interactions)- MANOVA

-   nmds (look for some groupings)?

-   shouldn't be any differences because they all spawn in the same area- Gulf of Guniea

```{r}

```
